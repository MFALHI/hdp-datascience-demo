---
title: "Airline Modeling Demo"
author: "Ofer Mendelevitch"
date: "11/7/2014"
output: html_document
---

# Data Science Demo - Hortonworks
# Modeling airline delays with Hadoop and R  

In this 3rd part of the demo on data science, we continue to demonstrate how to build a predictive model with Hadoop, this time we'll use R and RCloud.

[R](http://www.r-project.org/) is a language and environment for statistical computing and graphics. It is a GNU project which was developed at Bell Laboratories by John Chambers and colleagues. R is a powerful environment and very popular with data scientists. R is open source and has more than 5000 packages available, which make it an extremely powerful and mature environment.

[RCloud](https://github.com/att/rcloud) is an open-source project from AT&T labs, for collaboratively developing and sharing R code.

Recall from the first demo  that we are constructing a predictive model for flight delays. Our source dataset resides here: http://stat-computing.org/dataexpo/2009/the-data.html, and includes details about flights in the US from the years 1987-2008. We will also enrich the data with weather information from: http://www.ncdc.noaa.gov/cdo-web/datasets/, where we find daily temperatures (min/max), wind speed, snow conditions and precipitation. We will build a supervised learning model to predict flight delays for flights leaving O'Hare International airport (ORD). We will use the year 2007 data to build the model, and test it's validity using data from 2008.

```{r, message=FALSE, warning=FALSE}
# load required R packages
require(rhdfs)
require(randomForest)
require(gbm)
require(plyr)
require(data.table)
require(foreach)

# Initialize RHDFS
hdfs.init(hadoop='/usr/bin/hadoop')

# Utility function to convert celcius to fahrenheit
fahrenheit <- function(x) { return (x*1.8 + 32.0) }

# Extract hour of day from 3 or 4 digit time-of-day string
get_hour <- function(x) { 
  s = sprintf("%04d", as.numeric(x))
  return(substr(s, 1, 2))
}

#Utility function to read a multi-part file from HDFS into an R data frame
read_csv_from_hdfs <- function(filename, cols=NULL) {
  dir.list = hdfs.ls(filename)
  list.condition <- sapply(dir.list$size, function(x) x > 0)
  file.list  <- dir.list[list.condition,]
  tables <- lapply(file.list$file, function(f) {
    content <- paste(hdfs.read.text.file(f, buffer=100000000), collapse='\n')
    if (length(cols)==0) {
      dt = fread(content, sep=",", colClasses="character", stringsAsFactors=F, header=T)   
    } else {
      dt = fread(content, sep=",", colClasses="character", stringsAsFactors=F, header=F)   
      setnames(dt, names(dt), cols)    
    }
    dt
  })
  rbind.fill(tables)
}
```

## Data Exploration
First we explore the dataset to determine which variables are reasonable to use for this prediction task. We will take a look at the year 2007 as an example:

```{r, message=FALSE, results="hide"}
# read 2007 year file
cols = c('year', 'month', 'day', 'dow', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime','Carrier', 'FlightNum', 'TailNum', 'ActualElapsedTime', 'CRSElapsedTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest', 'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'CancellationCode', 'Diverted', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay');
flt_2007 = read_csv_from_hdfs('/user/demo/airline/delay/2007.csv', cols)

print(dim(flt_2007))
```
So we have 7.4M+ flights in 2007 and 29 variables.

Our "target" variable will be *DepDelay* (scheduled departure delay in minutes). To build a classifier, we further refine our target variable into a binary variable by defining a "delay" as having 15 mins or more of delay, and "non-delay" otherwise. We thus create a new binary variable that we name *'DepDelayed'*.

Let's look at some basic statistics, after limiting ourselves to flights originating from ORD:

```{r, results="hold"}
# Basic exploration
df1 = flt_2007[which(flt_2007$Origin == 'ORD' & !is.na(flt_2007$DepDelay)),]
df1$DepDelay = sapply(df1$DepDelay, function(x) (if (as.numeric(x)>=15) 1 else 0))

print(paste0("total flights: ", as.character(dim(df1)[1])))
print(paste0("total delays: ", as.character(sum(df1$DepDelay))))
```

Now let's look at delays by month:

```{r}
df2 = df1[, c('DepDelay', 'month'), with=F]
df2$month = as.numeric(df2$month)
df2 <- ddply(df2, .(month), summarise, mean_delay=mean(DepDelay))
barplot(df2$mean_delay, names.arg=df2$month, xlab="month", ylab="% of delays", col="blue")
```

And delays by hour of day:

```{r}
df2 = df1[, c('DepDelay', 'CRSDepTime'), with=F]
df2$hour = as.numeric(sapply(df2$CRSDepTime, get_hour))
df2$CRSDepTime <- NULL
df2 <- ddply(df2, .(hour), summarise, mean_delay=mean(DepDelay))
barplot(df2$mean_delay, names.arg=df2$hour, xlab="hour of day", ylab="% of delays", col="green")
```

Exploring the dataset is the first step before modeling. We are trying to figure out which features of the model might be good predictors, and how they behave: distribution, range, etc.

In this demo we have not explored all the variables of course, just a couple. Now let's continue to demonstrate how to use some R packages for building a predictive model.

***
## Pre-processing

We perform the same data pre-processing we did in part 1, iteration 3 - using PIG to create our feature matrix, which includes the following variables:

* **month**: winter months should have more delays than summer months
* **day of month**: this is likely not a very predictive variable, but let's keep it in anyway
* **day of week**: weekend vs. weekday
* **hour of the day**: later hours tend to have more delays
* **Distance**: interesting to see if this variable is a good predictor of delay
* **days_from_closest_holiday**: number of days from date of flight to closest US holiday
* **max_temp**: highest temprature at ORD on the day of the flight
* **min_temp**: lowest temprature at ORD on the day of the flight
* **precipitation**: precipitation at ORD on the day of the flight
* **wind_speed**: wind speed at ORD on the day of the flight
* **snow**: snow level at ORD on the day of the flight

As a reminder, we use some Python UDFs defined by util.py:
```

```{r, engine='python'}
#
# Python UDFs for our PIG script
#
from datetime import date

# get hour-of-day from HHMM field
def get_hour(val):
  return int(val.zfill(4)[:2])

# this array defines the dates of holiday in 2007 and 2008
holidays = [
        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \
        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25), \
        date(2008, 1, 1), date(2008, 1, 21), date(2008, 2, 18), date(2008, 5, 22), date(2008, 5, 26), date(2008, 7, 4), \
        date(2008, 9, 1), date(2008, 10, 13), date(2008, 11, 11), date(2008, 11, 27), date(2008, 12, 25) \
     ]
# get number of days from nearest holiday
def days_from_nearest_holiday(year, month, day):
  d = date(year, month, day)
  x = [(abs(d-h)).days for h in holidays]
  return min(x)
```


```{r}
pig_script = "
register 'util.py' USING jython as util;

-- Helper macro to load data and join into a feature vector per instance
DEFINE preprocess(year_str, airport_code) returns data
{
    -- load airline data from specified year (need to specify fields since it's not in HCat)
    airline = load 'airline/delay/$year_str.csv' using PigStorage(',') 
                    as (Year: int, Month: int, DayOfMonth: int, DayOfWeek: int, DepTime: chararray, CRSDepTime:chararray, 
                        ArrTime, CRSArrTime, Carrier: chararray, FlightNum, TailNum, ActualElapsedTime, CRSElapsedTime, AirTime, 
                        ArrDelay, DepDelay: int, Origin: chararray, Dest: chararray, Distance: int, TaxiIn, TaxiOut, 
                        Cancelled: int, CancellationCode, Diverted, CarrierDelay, WeatherDelay, NASDelay, 
                        SecurityDelay, LateAircraftDelay);

    -- keep only instances where flight was not cancelled and originate at ORD
    airline_flt = filter airline by Cancelled == 0 and Origin == '$airport_code';

    -- Keep only fields I need
    airline2 = foreach airline_flt generate Year as year, Month as month, DayOfMonth as day, DayOfWeek as dow,
                        Carrier as carrier, Origin as origin, Dest as dest, Distance as distance,
                        CRSDepTime as time, DepDelay as delay, util.to_date(Year, Month, DayOfMonth) as date;

    -- load weather data
    weather = load 'airline/weather/$year_str.csv' using PigStorage(',') 
                    as (station: chararray, date: chararray, metric, value, t1, t2, t3, time);

    -- keep only TMIN and TMAX weather observations from ORD
    weather_tmin = filter weather by station == 'USW00094846' and metric == 'TMIN';
    weather_tmax = filter weather by station == 'USW00094846' and metric == 'TMAX';
    weather_prcp = filter weather by station == 'USW00094846' and metric == 'PRCP';
    weather_snow = filter weather by station == 'USW00094846' and metric == 'SNOW';
    weather_awnd = filter weather by station == 'USW00094846' and metric == 'AWND';

    joined = join airline2 by date, weather_tmin by date, weather_tmax by date, weather_prcp by date, 
                                    weather_snow by date, weather_awnd by date;
    $data = foreach joined generate delay, month, day, dow, util.get_hour(airline2::time) as tod, distance, carrier, dest,
                                    util.days_from_nearest_holiday(year, month, day) as hdays,
                                    weather_tmin::value as temp_min, weather_tmax::value as temp_max,
                                    weather_prcp::value as prcp, weather_snow::value as snow, weather_awnd::value as wind;
};

ORD_2007 = preprocess('2007', 'ORD');
rmf airline/fm/ord_2007_R;
store ORD_2007 into 'airline/fm/ord_2007_R' using PigStorage(',');

ORD_2008 = preprocess('2008', 'ORD');
rmf airline/fm/ord_2008_R;
store ORD_2008 into 'airline/fm/ord_2008_R' using PigStorage(',');
"
output = system2("pig", 
                 input=pig_script,
                 wait=TRUE)
```

***
## Modeling

Finally, let's use the resulting feature matrix in R, to build a predictive model for predicting airline delays.

First we prepare our trainning set and test set:
The preprocess_data function reads the data from HDFS into an R data frame *train_data* for the training set. The variable *test_x* includes the testing feature matrix, and separately the target variable vector *test_y*.

Here we also define a helper function *get_metrics* that we will use later to measure precision, recall, F1 and accuracy.

```{r, message=FALSE}
# Function to compute Precision, Recall and F1-Measure
get_metrics <- function(predicted, actual) {
  tp = length(which(predicted == TRUE & actual == TRUE))
  tn = length(which(predicted == FALSE & actual == FALSE))
  fp = length(which(predicted == TRUE & actual == FALSE))
  fn = length(which(predicted == FALSE & actual == TRUE))

  precision = tp / (tp+fp)
  recall = tp / (tp+fn)
  F1 = 2*precision*recall / (precision+recall)
  accuracy = (tp+tn) / (tp+tn+fp+fn)
  
  v = c(precision, recall, F1, accuracy)
  v
}

# Read input files
process_dataset <- function(filename) {
  cols = c('delay', 'month', 'day', 'dow', 'hour', 'distance', 'carrier', 'dest', 
        'days_from_holiday', 'origin_tmin', 'origin_tmax', 'origin_prcp', 'origin_snow', 'origin_wind')

  data = read_csv_from_hdfs(filename, cols)
  data$origin_tmin = sapply(data$origin_tmin, function(x) fahrenheit(as.numeric(x)/10.0))
  data$origin_tmax = sapply(data$origin_tmax, function(x) fahrenheit(as.numeric(x)/10.0))
  data$origin_prcp = sapply(data$origin_prcp, as.numeric)
  data$origin_snow = sapply(data$origin_snow, as.numeric)
  data$origin_wind = sapply(data$origin_wind, as.numeric)
  data$distance = as.numeric(data$distance)
  data$delay = as.factor(as.numeric(data$delay) >= 15)
  data
}

topK <- function(x,k){
  x <- as.factor(x)
  tbl <- tabulate(x)
  names(tbl) <- levels(x)
  x <- as.character(x)
  levelsToKeep <- names(tail(sort(tbl),k))
  x[!(x %in% levelsToKeep)] <- 'rest'
  factor(x)
}

recode.categ <- function(df) {
  df$month = as.factor(df$month)
  df$day = as.factor(df$day)
  df$dow = as.factor(df$dow)
  df$hour = as.factor(df$hour)
  df$days_from_holiday = as.numeric(df$hour)
  df$dest = topK(df$dest, 25)
  df$carrier = topK(df$carrier, 25) 
  categ = lapply(df[,c("dest", "carrier")], as.factor)
  bin_df = model.matrix(~ . -1, data=categ, contrasts.arg = lapply(categ, contrasts, contrasts=FALSE))
  df$dest <- NULL
  df$carrier <- NULL
  out = cbind(df, bin_df)
  out
}

# Prepare training set and test/validation set

data_2007 = process_dataset('/user/demo/airline/fm/ord_2007_R')
train_data = recode.categ(data_2007)

data_2008 = process_dataset('/user/demo/airline/fm/ord_2008_R')
test_data = recode.categ(data_2008)

cols = intersect(names(train_data), names(test_data))
train_data = train_data[,cols]
test_data = test_data[,cols]

train_y = train_data$delay
train_x = train_data
train_x$delay <- NULL
test_y = test_data$delay
test_x = test_data
test_x$delay <- NULL
```

Now let's run R's random forest algorithm and evaluate the results.
Note that since R's randomForest package does not support parallelization natively, we use the [foreach](http://cran.r-project.org/web/packages/foreach/index.html) package to parallelize learning over multiple cores:

```{r, results='hold'}  
#rf <- randomForest(delay ~ ., data=train_data, ntree=40)
rf.model <- foreach(nt=rep(20, 4), .combine=combine) %do% randomForest(train_x, train_y, ntree=nt)
rf.pr <- predict(rf.model, newdata=test_x)
m.rf = get_metrics(as.logical(rf.pr), as.logical(test_y))
print(sprintf("Random Forest: precision=%0.2f, recall=%0.2f, F1=%0.2f, accuracy=%0.2f", m.rf[1], m.rf[2], m.rf[3], m.rf[4]))
```

Let's also try R's Gradient Boosted Machines (GBM) modeling. 
GBM is an ensemble method that like random forest is typically robust to over-fitting.

```{r, results='hold'}
gbm.model <- gbm.fit(train_x, as.numeric(train_y)-1, n.trees=500, verbose=F, shrinkage=0.01, distribution="bernoulli", 
                     interaction.depth=3, n.minobsinnode=30)
gbm.pr <- predict(gbm.model, newdata=test_x, n.trees=500, type="response")
m.gbm = get_metrics(gbm.pr >= 0.5, as.logical(test_y))
print(sprintf("Gradient Boosted Machines: precision=%0.2f, recall=%0.2f, F1=%0.2f, accuracy=%0.2f", m.gbm[1], m.gbm[2], m.gbm[3], m.gbm[4]))
```

As we can see, both Random Forest and GBM provide pretty good results.

## Summary
In this blog post we have demonstrated how to build a predictive model with Hadoop and R. We have used R to explore our raw dataset, and then used Random Forest and Logistic regression modeling applied to the feature matrix already created in the first part of this blog using PIG.

