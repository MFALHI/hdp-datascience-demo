{
 "metadata": {
  "name": "",
  "signature": "sha256:effbb48d6d0b399cf1a9546988adc6912adf231bd3d02342d467ac04bdb70c14"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Science with Hadoop - predicting airline delays - part 2: Spark"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this 2nd part of the blog post, we continue to demonstrate how to build a predictive model with Hadoop, this time we'll use Spark and ML-Lib.\n",
      "\n",
      "Apache Spark is a relatively new entrant to the Hadoop ecosystem. Running on YARN, Apache Spark is an in-memory data processing API and execution engine that is effective for machine learning and data science use cases along side other workloads.\n",
      "\n",
      "In the context of our demo, we will show how to use Apache Spark via its PySpark API to generate our feature matrix and also use ML-Lib (Spark's machine learning library) to build and evaluate models.\n",
      "\n",
      "Recall from part 1 that we are exploring a predictive model for flight delays. Our source dataset resides here: http://stat-computing.org/dataexpo/2009/the-data.html, and includes details about flights in the US from the years 1987-2008. We have also enriched the data with weather information from: http://www.ncdc.noaa.gov/cdo-web/datasets/, where we find daily temperatures (min/max), wind speed, snow conditions and precipitation. \n",
      "\n",
      "We will build a supervised learning model to predict flight delays for flights leaving O'Hare International airport (ORD). We will use the year 2007 data to build the model, and test it's validity using data from 2008.\n",
      "\n",
      "So let's begin."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Pre-processing with Hadoop, and PySpark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apache Spark's basic data abstraction is that of an RDD (resilient distributed dataset), which is a fault-tolerant collection of elements that can be operated on in parallel across your Hadoop cluster. \n",
      "\n",
      "Spark's API (available in Scala, Python or Java) supports a variety of operations such as map() and flatMap(), filter(), join(), and more. For a full description of the API please check the Spark API programming guide: http://spark.apache.org/docs/1.1.0/programming-guide.html \n",
      "\n",
      "We will show how to perform the same pre-processing with Spark (using its Python API - PySpark) as we did with PIG previously. First, let's define a few Python functions for feature generation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import date\n",
      "holidays = [\n",
      "        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n",
      "        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25), \\\n",
      "        date(2008, 1, 1), date(2008, 1, 21), date(2008, 2, 18), date(2008, 5, 22), date(2008, 5, 26), date(2008, 7, 4), \\\n",
      "        date(2008, 9, 1), date(2008, 10, 13), date(2008, 11, 11), date(2008, 11, 27), date(2008, 12, 25) \\\n",
      "     ]\n",
      "\n",
      "def get_hour(val): return(int(val.zfill(4)[:2]))\n",
      "def days_from_nearest_holiday(year, month, day):\n",
      "  d = date(year, month, day)\n",
      "  x = [(abs(d-h)).days for h in holidays]\n",
      "  return min(x)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": []
      },
      {
       "ename": "",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "<console>:1: error: ';' expected but 'import' found.",
        "       from datetime import date",
        "                     ^",
        "<console>:2: error: illegal start of simple expression",
        "       holidays = [",
        "                  ^",
        "<console>:9: error: identifier expected but 'val' found.",
        "       def get_hour(val): return(int(val.zfill(4)[:2]))",
        "                    ^"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": []
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we use Spark to create the simple feature matrices from iteration #1. \n",
      "\n",
      "The python function gen_features(row) takes a row of input and generates a comma-separated string with all the features.\n",
      "preprocess_spark() performs the complete pre-processing task using Spark on a given file. In our case we need to do this for both the 2007 file (our training set) and 2008 file (our validation/test set)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pydoop.hdfs as hdfs\n",
      "\n",
      "fields = [\"Year\", \"Month\", \"Day\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"ArrTime\", \"CRSArrTime\", \"Carrier\", \n",
      "          \"FlightNum\", \"TailNum\", \"ActualElapsedTime\", \"CRSElapsedTime\", \"AirTime\", \"ArrDelay\", \"Delay\", \n",
      "          \"Origin\", \"Dest\", \"Distance\", \"TaxiIn\", \"TaxiOut\", \"Cancelled\", \"CancellationCode\", \"Diverted\", \n",
      "          \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\"]\n",
      "\n",
      "def gen_features(row):\n",
      "    f1 = [row[fields.index(x)] for x in ['Delay', 'Month', 'Day', 'DayOfWeek']]\n",
      "    f2 = [get_hour(row[fields.index('DepTime')])]\n",
      "    f3 = [row[fields.index('Distance')]]\n",
      "#    f3 = [row[fields.index(x)] for x in ['Distance', 'Carrier', 'Dest']]\n",
      "    year = int(row[fields.index('Year')])\n",
      "    month = int(row[fields.index('Month')])\n",
      "    day = int(row[fields.index('Day')])\n",
      "    f4 = [days_from_nearest_holiday(year, month, day)]\n",
      "    flist = f1 + f2 + f3 + f4\n",
      "    return ','.join([str(x) for x in flist])\n",
      "\n",
      "# function to do a preprocessing step for a given file\n",
      "def preprocess_spark(infile):\n",
      "  lines = sc.textFile(infile)\n",
      "  header = ','.join(lines.take(1))\n",
      "  data = lines.filter(lambda line: line != header) \\\n",
      "              .map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[fields.index('Cancelled')] == \"0\") \\\n",
      "              .filter(lambda vals: vals[fields.index('Origin')] == 'ORD')\n",
      "  results = data.map(gen_features)\n",
      "  return results\n",
      "\n",
      "data_2007 = preprocess_spark('airline/delay/2007.csv')\n",
      "#outfile = 'airline/fm-spark/ord_2007_1'\n",
      "#if hdfs.path.exists(outfile): \n",
      "#  hdfs.rmr(outfile)\n",
      "#data_2007.saveAsTextFile(outfile)\n",
      "\n",
      "#print data_2007.take(5)\n",
      "\n",
      "data_2008 = preprocess_spark('airline/delay/2008.csv')\n",
      "#outfile = 'airline/fm-spark/ord_2008_1'\n",
      "#if hdfs.path.exists(outfile): \n",
      "#  hdfs.rmr(outfile)\n",
      "#data_2008.saveAsTextFile(outfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['-8,1,25,4,10,719,10', '41,1,28,7,15,925,13', '45,1,29,1,20,316,14', '-9,1,17,3,18,719,2', '180,1,12,5,20,316,3']\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we keep data_2007 and data_2008 as Spark RDDs. We could save them back to HDFS, but we can continue to use them directly as RDDs."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Modeling with Spark and ML-Lib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the training and validation datasets ready, let's see how to build a predictive model with Spark's ML-Lib machine learning library.\n",
      "\n",
      "MLlib is Spark\u2019s scalable machine learning library, which includes various learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, and others. \n",
      "\n",
      "If you compare ML-Lib to Scikit-learn, at the moment ML-Lib lacks a few important algorithms like Random Forest, Gradient Boosted Trees, and others. nevertheless, it's a strong library that runs natively on Spark, with a strong community behind it.\n",
      "\n",
      "Let's try a few algorithms from ML-Lib on our dataset.\n",
      "First we parse our feature matrices into RDDs of LabeledPoint instances for both the training and test datasets. We also define \"Eval_metrics\" a helper function to compute precision, recall, F1 measure and accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.regression import LabeledPoint\n",
      "\n",
      "# Load and parse the data\n",
      "def parsePoint(line):\n",
      "    values = [float(x) for x in line.split(',')]\n",
      "    return LabeledPoint(1 if values[0]>=15 else 0, values[1:])\n",
      "\n",
      "# Evaluating the model's performance\n",
      "def eval_metrics(labelsAndPreds):\n",
      "    tp = float(labelsAndPreds.filter(lambda r: r[0]==1 and r[1]==1).count())\n",
      "    tn = float(labelsAndPreds.filter(lambda r: r[0]==0 and r[1]==0).count())\n",
      "    fp = float(labelsAndPreds.filter(lambda r: r[0]==0 and r[1]==1).count())\n",
      "    fn = float(labelsAndPreds.filter(lambda r: r[0]==1 and r[1]==0).count())\n",
      "\n",
      "    print [tp, tn, fp, fn]\n",
      "    \n",
      "    precision = tp / (tp+fp)\n",
      "    recall = tp / (tp+fn)\n",
      "    F_measure = 2*precision*recall / (precision+recall)\n",
      "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
      "    return {'precision': precision, 'recall': recall, 'F1': F_measure, 'accuracy': accuracy}\n",
      "\n",
      "# Prepare training set\n",
      "##train_data = sc.textFile('airline/fm-spark/ord_2007_2')\n",
      "parsedTrainData = data_2007.map(parsePoint)\n",
      "\n",
      "# Prepare test set\n",
      "##test_data = sc.textFile('airline/fm-spark/ord_2008_2')\n",
      "parsedTestData = data_2008.map(parsePoint)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ML-Lib supports linear regression and its classification variants, implemented via Stochastic Gradient descent (SGD).\n",
      "Let's see how to use it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
      "\n",
      "# Build the LR model\n",
      "model_lr = LogisticRegressionWithSGD.train(parsedTrainData, iterations=100, regParam=25.0)\n",
      "\n",
      "# Predict\n",
      "labelsAndPreds = parsedTestData.map(lambda p: (p.label, model_lr.predict(p.features)))\n",
      "m = eval_metrics(labelsAndPreds)\n",
      "print (\"precision = %0.2f, recall= %0.2f, F1 = %0.2f, accuracy = %0.2f\" % (m['precision'], m['recall'], m['F1'], m['accuracy']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[95436.0, 0.0, 239894.0, 0.0]\n",
        "precision = 0.28, recall= 1.00, F1 = 0.44, accuracy = 0.28\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now let's try SVM with SGD:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from pyspark.mllib.classification import SVMWithSGD\n",
      "\n",
      "# Build the SVM model\n",
      "model_svm = SVMWithSGD.train(parsedTrainData, iterations=100)\n",
      "\n",
      "# Predict\n",
      "labelsAndPreds = parsedTestData.map(lambda p: (p.label, model_svm.predict(p.features)))\n",
      "m = eval_metrics(labelsAndPreds)\n",
      "print (\"precision = %0.2f, recall= %0.2f, F1 = %0.2f, accuracy = %0.2f\" % (m['precision'], m['recall'], m['F1'], m['accuracy']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.0, 239894.0, 0.0, 95436.0]\n"
       ]
      },
      {
       "ename": "ZeroDivisionError",
       "evalue": "float division by zero",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-45-8a675e5143bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'\\nfrom pyspark.mllib.classification import SVMWithSGD\\n\\n# Build the SVM model\\nmodel_svm = SVMWithSGD.train(parsedTrainData, iterations=100)\\n\\n# Predict\\nlabelsAndPreds = parsedTestData.map(lambda p: (p.label, model_svm.predict(p.features)))\\nm = eval_metrics(labelsAndPreds)\\nprint (\"precision = %0.2f, recall= %0.2f, F1 = %0.2f, accuracy = %0.2f\" % (m[\\'precision\\'], m[\\'recall\\'], m[\\'F1\\'], m[\\'accuracy\\']))'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/demo/pyenv/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2160\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2161\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2162\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2163\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/demo/pyenv/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
        "\u001b[1;32m/home/demo/pyenv/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/demo/pyenv/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1127\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m<ipython-input-43-7e17306a3d02>\u001b[0m in \u001b[0;36meval_metrics\u001b[1;34m(labelsAndPreds)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mF_measure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecall\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Pre-processing: Iteration 3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TBD: don't stop here. Add weather and binary dummy variables\n",
      "\n",
      "\n",
      "Recall that in part 1, we decided to enrich the dataset by integrating weather, and we've seen that modeling produces better results with the additional features. Let's implement this enhanced feature matrix generation as well here, again using Apache Spark.\n",
      "\n",
      "Here we need to join the two datasets, by the date field. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time \n",
      "\n",
      "delay_fields = [ \"Year\", \"Month\", \"Day\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"ArrTime\", \"CRSArrTime\", \"Carrier\", \\\n",
      "            \"FlightNum\", \"TailNum\", \"ActualElapsedTime\", \"CRSElapsedTime\", \"AirTime\", \"ArrDelay\", \"Delay\", \\\n",
      "            \"Origin\", \"Dest\", \"Distance\", \"TaxiIn\", \"TaxiOut\", \"Cancelled\", \"CancellationCode\", \"Diverted\", \\\n",
      "            \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\" ]\n",
      "\n",
      "year_inx = delay_fields.index('Year')\n",
      "month_inx = delay_fields.index('Month')\n",
      "day_inx = delay_fields.index('Day')\n",
      "\n",
      "weather_fields = [ \"station\", \"date\", \"metric\", \"value\", \"t1\", \"t2\", \"t3\", \"t4\" ]\n",
      "\n",
      "date_inx = weather_fields.index('date')\n",
      "metric_inx = weather_fields.index('metric')\n",
      "value_inx = weather_fields.index('value')\n",
      "\n",
      "def gen_features(row):\n",
      "    f1 = [row[delay_fields.index(x)] for x in ['Delay', 'Year', 'Month', 'Day', 'DayOfWeek']]\n",
      "    f2 = [get_hour(row[delay_fields.index('DepTime')])]\n",
      "    f3 = [row[delay_fields.index('Distance')]]\n",
      "    year = int(row[year_inx])\n",
      "    month = int(row[month_inx])\n",
      "    day = int(row[day_inx])\n",
      "    f4 = [is_bad_carrier(row[delay_fields.index('Carrier')]), is_bad_dest(row[delay_fields.index('Dest')]), \\\n",
      "          days_from_nearest_holiday(year, month, day)]\n",
      "    flist = f1 + f2 + f3 + f4\n",
      "    return (to_date(year, month, day), flist)\n",
      "\n",
      "def to_date(year, month, day):\n",
      "  s = \"%04d%02d%02d\" % (year, month, day)\n",
      "  return s\n",
      "\n",
      "# function to do a preprocessing step for a given file\n",
      "def preprocess_spark(delay_file, weather_file):\n",
      "  \n",
      "  # Read airline delay dataset\n",
      "  lines = sc.textFile(delay_file)\n",
      "  header = ','.join(lines.take(1))\n",
      "  data = lines.filter(lambda line: line != header) \\\n",
      "              .map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[delay_fields.index('Cancelled')] == \"0\") \\\n",
      "              .filter(lambda vals: vals[delay_fields.index('Origin')] == 'ORD')\n",
      "  res_delay = data.map(gen_features)\n",
      "                \n",
      "  # Read weather data into RDDs\n",
      "  lines = sc.textFile(weather_file)\n",
      "  data = lines.map(lambda line: line.split(\",\")) \\\n",
      "              .filter(lambda vals: vals[weather_fields.index('station')] == 'USW00094846')\n",
      "  w_tmin = data.filter(lambda vals: vals[metric_inx] == 'TMIN') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_tmax = data.filter(lambda vals: vals[metric_inx] == 'TMAX') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_prcp = data.filter(lambda vals: vals[metric_inx] == 'PRCP') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_snow = data.filter(lambda vals: vals[metric_inx] == 'SNOW') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "  w_awnd = data.filter(lambda vals: vals[metric_inx] == 'AWND') \\\n",
      "              .map(lambda vals: (vals[date_inx], vals[value_inx]))\n",
      "                    \n",
      "  # Join weather data with delay data\n",
      "  joined = res_delay.join(w_tmin).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_tmax).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_prcp).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_snow).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .join(w_awnd).mapValues(lambda x: x[0]+[x[1]]) \\\n",
      "                    .map(lambda pair: ','.join([str(x) for x in pair[1]]))     \n",
      "\n",
      "  return joined\n",
      "\n",
      "data_2007 = preprocess_spark('airline/delay/2007.csv', 'airline/weather/2007.csv')\n",
      "outfile = 'airline/fm-spark/ord_2007_2'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2007.saveAsTextFile(outfile)\n",
      "print \"Finished pre-processing for 2007 datasets\"\n",
      "\n",
      "data_2008 = preprocess_spark('airline/delay/2008.csv', 'airline/weather/2008.csv')\n",
      "outfile = 'airline/fm-spark/ord_2008_2'\n",
      "if hdfs.path.exists(outfile): \n",
      "  hdfs.rmr(outfile)\n",
      "data_2008.saveAsTextFile(outfile)\n",
      "print \"Finished pre-processing for 2008 datasets\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Modeling with Spark and ML-Lib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the training and validation datasets ready, let's see how to build a predictive model with Spark's ML-Lib machine learning library.\n",
      "\n",
      "MLlib is Spark\u2019s scalable machine learning library, which includes various learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, and others. \n",
      "\n",
      "If you compare ML-Lib to Scikit-learn, at the moment ML-Lib lacks a few important algorithms like Random Forest, Gradient Boosted Trees, and others. But nonetheless it's a strong library with a bright future. \n",
      "\n",
      "Let's run a few predictive models with ML-Lib. First we parse our feature matrices into RDDs of LabeledPoint instances for both the training and test datasets. We also define \"Eval_metrics\" a helper function to compute precision, recall, F1 measure and accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.regression import LabeledPoint\n",
      "\n",
      "# Load and parse the data\n",
      "def parsePoint(line):\n",
      "    values = [float(x) for x in line.split(',')]\n",
      "    return LabeledPoint(1 if values[0]>=15 else 0, [float(x) for x in values[1:]])\n",
      "\n",
      "# Evaluating the model's performance\n",
      "def eval_metrics(labelsAndPreds):\n",
      "    tp = float(labelsAndPreds.filter(lambda r: r[0]==1 and r[1]==1).count())\n",
      "    tn = float(labelsAndPreds.filter(lambda r: r[0]==0 and r[1]==0).count())\n",
      "    fp = float(labelsAndPreds.filter(lambda r: r[0]==0 and r[1]==1).count())\n",
      "    fn = float(labelsAndPreds.filter(lambda r: r[0]==1 and r[1]==0).count())\n",
      "\n",
      "    precision = tp / (tp+fp)\n",
      "    recall = tp / (tp+fn)\n",
      "    F_measure = 2*precision*recall / (precision+recall)\n",
      "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
      "    return {'precision': precision, 'recall': recall, 'F1': F_measure, 'accuracy': accuracy}\n",
      "\n",
      "# Prepare training set\n",
      "train_data = sc.textFile('airline/fm-spark/ord_2007_2')\n",
      "parsedTrainData = train_data.map(parsePoint)\n",
      "\n",
      "# Prepare test set\n",
      "test_data = sc.textFile('airline/fm-spark/ord_2008_2')\n",
      "parsedTestData = test_data.map(parsePoint)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ML-Lib supports linear regression and its classification variants, implemented via Stochastic Gradient descent (SGD).\n",
      "Let's see how to use it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
      "\n",
      "# Build the LR model\n",
      "model_lr = LogisticRegressionWithSGD.train(parsedTrainData, iterations=100)\n",
      "\n",
      "# Predict\n",
      "labelsAndPreds = parsedTestData.map(lambda p: (p.label, model_lr.predict(p.features)))\n",
      "m = eval_metrics(labelsAndPreds)\n",
      "print (\"precision = %0.2f, recall= %0.2f, F1 = %0.2f, accuracy = %0.2f\" % (m['precision'], m['recall'], m['F1'], m['accuracy']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "precision = 0.35, recall= 0.55, F1 = 0.43, accuracy = 0.59\n",
        "CPU times: user 515 ms, sys: 216 ms, total: 731 ms\n",
        "Wall time: 1min 2s\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now let's try the Support Vector Machine version of ML-Lib:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from pyspark.mllib.classification import SVMWithSGD\n",
      "\n",
      "# Build the SVM model\n",
      "model_svm = SVMWithSGD.train(parsedTrainData, iterations=200)\n",
      "\n",
      "# Predict\n",
      "labelsAndPreds = parsedTestData.map(lambda p: (p.label, model_svm.predict(p.features)))\n",
      "m = eval_metrics(labelsAndPreds)\n",
      "print (\"precision = %0.2f, recall= %0.2f, F1 = %0.2f, accuracy = %0.2f\" % (m['precision'], m['recall'], m['F1'], m['accuracy']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "precision = 0.28, recall= 1.00, F1 = 0.44, accuracy = 0.28\n",
        "CPU times: user 876 ms, sys: 393 ms, total: 1.27 s\n",
        "Wall time: 1min 21s\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}